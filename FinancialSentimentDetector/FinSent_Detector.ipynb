{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08278e40-fb13-4dea-91da-8861379dfc2d",
   "metadata": {},
   "source": [
    "FinSent Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e20ec729-d5f0-4d3d-86b8-b7a2a99a4a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from scipy.special import softmax\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.nn.functional import softmax\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94a67dd1-cfd3-4024-8093-cf44720f56e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"yiyanghkust/finbert-tone\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"yiyanghkust/finbert-tone\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "801397cd-53c4-435a-9576-73541d040bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla's revenue increased by 40% this quarter. Analysts are optimistic.\n",
      "â†’ positive: 1.0000, neutral: 0.0000, negative: 0.0000\n",
      "\n",
      "TSLA stock is facing serious challenges due to poor delivery numbers.\n",
      "â†’ positive: 0.0000, neutral: 0.0000, negative: 1.0000\n",
      "\n",
      "Get ready! Tesla will skyrocket tomorrow!\n",
      "â†’ positive: 0.0015, neutral: 0.9985, negative: 0.0000\n",
      "\n",
      "Buy now or miss the rally!\n",
      "â†’ positive: 0.9913, neutral: 0.0048, negative: 0.0039\n",
      "\n"
     ]
    }
   ],
   "source": [
    "texts = [\n",
    "    \"Tesla's revenue increased by 40% this quarter. Analysts are optimistic.\",\n",
    "    \"TSLA stock is facing serious challenges due to poor delivery numbers.\",\n",
    "    \"Get ready! Tesla will skyrocket tomorrow!\",\n",
    "    \"Buy now or miss the rally!\",\n",
    "]\n",
    "\n",
    "for text in texts:\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=128)\n",
    "    outputs = model(**inputs)\n",
    "    probs = softmax(outputs.logits, dim=1)[0]\n",
    "    print(f\"{text}\\nâ†’ positive: {probs[1]:.4f}, neutral: {probs[0]:.4f}, negative: {probs[2]:.4f}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa09e91b-0ec0-450d-8e3c-fea63143d8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¿˜æ˜¯ä¸å¤Ÿå‡†ç¡®ï¼ˆskyrocketé‚£å¥ï¼‰\n",
    "# åŠ è½½ä¸¤ä¸ªæ¨¡å‹ï¼ˆFinBERT å’Œ Twitter-RoBERTaï¼‰ï¼Œä»¥ä¾¿è¯†åˆ«è´¢æŠ¥å’Œæ•£æˆ·è¯­è¨€\n",
    "\n",
    "# FinBERTï¼ˆæ“…é•¿åˆ†æè´¢ç»æ–°é—»ï¼‰\n",
    "finbert_model = AutoModelForSequenceClassification.from_pretrained(\"yiyanghkust/finbert-tone\")\n",
    "finbert_tokenizer = AutoTokenizer.from_pretrained(\"yiyanghkust/finbert-tone\")\n",
    "\n",
    "# Twitter-RoBERTaï¼ˆæ“…é•¿ç¤¾äº¤åª’ä½“ã€å£å·å¼è¯­è¨€ï¼‰\n",
    "twitter_model = AutoModelForSequenceClassification.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment\")\n",
    "twitter_tokenizer = AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment\")\n",
    "\n",
    "# FinBERTæƒ…ç»ªåˆ†æ\n",
    "def get_finbert_sentiment(text):\n",
    "    inputs = finbert_tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=128)\n",
    "    outputs = finbert_model(**inputs)\n",
    "    probs = softmax(outputs.logits, dim=1)[0]\n",
    "    return {\"positive\": probs[1], \"neutral\": probs[0], \"negative\": probs[2]}\n",
    "\n",
    "# Twitter-RoBERTaæƒ…ç»ªåˆ†æ\n",
    "def get_twitter_sentiment(text):\n",
    "    inputs = twitter_tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=128)\n",
    "    outputs = twitter_model(**inputs)\n",
    "    probs = softmax(outputs.logits, dim=1)[0]\n",
    "    return {\"negative\": probs[0], \"neutral\": probs[1], \"positive\": probs[2]}  # æ³¨æ„é¡ºåºä¸åŒ\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "745fdb5a-150b-4d7f-b309-0aa719a5f529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç®€å•çš„åŠ æƒå¹³å‡\n",
    "\n",
    "def get_combined_sentiment(text, alpha=1):\n",
    "    finbert_scores = get_finbert_sentiment(text)\n",
    "    twitter_scores = get_twitter_sentiment(text)\n",
    "\n",
    "    combined = {\n",
    "        label: alpha * twitter_scores[label] + (1 - alpha) * finbert_scores[label]\n",
    "        for label in [\"positive\", \"neutral\", \"negative\"]\n",
    "    }\n",
    "    return combined\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5c0197b-250a-4214-b603-baac7cf45b33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla will skyrocket tomorrow!\n",
      "â†’ Positive: 0.8989, Neutral: 0.0963, Negative: 0.0048\n",
      "\n",
      "TSLA stock is facing serious risks.\n",
      "â†’ Positive: 0.0060, Neutral: 0.1632, Negative: 0.8308\n",
      "\n",
      "Revenue grew 30% this quarter, a strong signal.\n",
      "â†’ Positive: 0.9338, Neutral: 0.0651, Negative: 0.0011\n",
      "\n",
      "Get in now before it jumps!\n",
      "â†’ Positive: 0.2955, Neutral: 0.6035, Negative: 0.1011\n",
      "\n",
      "Tesla is probably overvalued.\n",
      "â†’ Positive: 0.0192, Neutral: 0.2151, Negative: 0.7657\n",
      "\n"
     ]
    }
   ],
   "source": [
    "texts = [\n",
    "    \"Tesla will skyrocket tomorrow!\",\n",
    "    \"TSLA stock is facing serious risks.\",\n",
    "    \"Revenue grew 30% this quarter, a strong signal.\",\n",
    "    \"Get in now before it jumps!\",\n",
    "    \"Tesla is probably overvalued.\",\n",
    "]\n",
    "\n",
    "for text in texts:\n",
    "    result = get_combined_sentiment(text)\n",
    "    print(f\"{text}\\nâ†’ Positive: {result['positive']:.4f}, Neutral: {result['neutral']:.4f}, Negative: {result['negative']:.4f}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efedad52-b24e-4aaa-bb7c-9e0c27f64806",
   "metadata": {},
   "source": [
    "åŠ æƒå¹³å‡ä¹Ÿä¸å‡†ç¡®ï¼Œå»ºç«‹èåˆæ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "937cf6a5-80a5-4c0f-9f19-f8715038aa42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            Sentence Sentiment\n",
      "0  The GeoSolutions technology will leverage Bene...  positive\n",
      "1  $ESI on lows, down $1.50 to $2.50 BK a real po...  negative\n",
      "2  For the last quarter of 2010 , Componenta 's n...  positive\n",
      "3  According to the Finnish-Russian Chamber of Co...   neutral\n",
      "4  The Swedish buyout firm has sold its remaining...   neutral\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\86153\\\\financial-sentiment-analysis\\\\data.csv\")\n",
    "\n",
    "df = df.head(1000)\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d24bf3fc-7fa0-4875-8273-cb1414174916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [06:41<00:00,  2.49it/s]\n"
     ]
    }
   ],
   "source": [
    "finbert_model.eval()\n",
    "twitter_model.eval()\n",
    "\n",
    "\n",
    "# 2. ç»Ÿä¸€æ ‡ç­¾é¡ºåºï¼š0 - negative, 1 - neutral, 2 - positive\n",
    "id2label = {0: \"negative\", 1: \"neutral\", 2: \"positive\"}\n",
    "\n",
    "# å› ä¸º FinBERT åŸå§‹é¡ºåºæ˜¯ {0: neutral, 1: positive, 2: negative}\n",
    "# æ‰€ä»¥æˆ‘ä»¬è¦å°†å®ƒè¾“å‡ºçš„å‘é‡ permute æˆ [negative, neutral, positive]\n",
    "def reorder_finbert_probs(probs):\n",
    "    return np.array([probs[2], probs[0], probs[1]])\n",
    "\n",
    "# 3. å®šä¹‰é¢„æµ‹å‡½æ•°\n",
    "def get_probs(text, tokenizer, model, reorder=False):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=128)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        probs = softmax(outputs.logits, dim=1).squeeze().numpy()\n",
    "    if reorder:\n",
    "        probs = reorder_finbert_probs(probs)\n",
    "    return probs\n",
    "\n",
    "# 4. å®šä¹‰èåˆå‡½æ•°ï¼ˆå¯åŠ¨æ€è°ƒæƒï¼‰\n",
    "def adjust_alpha(text):\n",
    "    text_lower = text.lower()\n",
    "\n",
    "    # åå‘ FinBERT çš„é‡‘èå…³é”®è¯\n",
    "    fin_keywords = [\n",
    "        \"stock\", \"market\", \"profit\", \"nasdaq\", \"fed\", \"earnings\",\n",
    "        \"inflation\", \"revenue\", \"guidance\", \"fomc\", \"qe\", \"rate hike\"\n",
    "    ]\n",
    "    \n",
    "    # åå‘ Twitter çš„ç½‘ç»œçƒ­è¯å’Œæƒ…ç»ªè¡¨è¾¾\n",
    "    emotion_keywords = [\n",
    "        \"wow\", \"awesome\", \"lol\", \"ğŸ˜­\", \"ğŸ”¥\", \"ğŸ’¥\", \"!\", \"omg\", \"insane\",\n",
    "        \"crazy\", \"skyrocketing\", \"to the moon\", \"crash\", \"plummet\", \"surge\",\n",
    "        \"collapse\", \"explode\", \"rally\", \"selloff\", \"moon\", \"ğŸš€\", \"lmao\"\n",
    "    ]\n",
    "\n",
    "    has_fin = any(word in text_lower for word in fin_keywords)\n",
    "    has_emotion = any(word in text_lower for word in emotion_keywords)\n",
    "\n",
    "    # ç®€åŒ–çš„åˆ¤æ–­é€»è¾‘\n",
    "    if has_fin and not has_emotion:\n",
    "        return 0.8  # åå‘ FinBERT\n",
    "    elif has_emotion and not has_fin:\n",
    "        return 0.3  # åå‘ Twitter\n",
    "    else:\n",
    "        return 0.5  # å«æ··æˆ–éƒ½ä¸åŒ…å«æ—¶ä¿æŒä¸­æ€§èåˆ\n",
    "\n",
    "def merge_probs(p1, p2, alpha):\n",
    "    return alpha * p1 + (1 - alpha) * p2\n",
    "    \n",
    "# 5. æ‰§è¡Œæ¨¡å‹é¢„æµ‹ä¸èåˆ\n",
    "finbert_probs = []\n",
    "twitter_probs = []\n",
    "merged_probs = []\n",
    "merged_labels = []\n",
    "merged_confidences = []\n",
    "\n",
    "print(\"Processing...\")\n",
    "\n",
    "for text in tqdm(df[\"Sentence\"]):\n",
    "    fb_p = get_probs(text, finbert_tokenizer, finbert_model, reorder=True)\n",
    "    tw_p = get_probs(text, twitter_tokenizer, twitter_model)\n",
    "    alpha = adjust_alpha(text)\n",
    "    merged_p = merge_probs(fb_p, tw_p, alpha)\n",
    "    label = id2label[np.argmax(merged_p)]\n",
    "    confidence = np.max(merged_p)\n",
    "    \n",
    "    finbert_probs.append(fb_p)\n",
    "    twitter_probs.append(tw_p)\n",
    "    merged_probs.append(merged_p)\n",
    "    merged_labels.append(label)\n",
    "    merged_confidences.append(confidence)\n",
    "\n",
    "# 6. åŠ å…¥è¾“å‡ºåˆ—\n",
    "df[\"finbert_probs\"] = finbert_probs\n",
    "df[\"twitter_probs\"] = twitter_probs\n",
    "df[\"merged_probs\"] = merged_probs\n",
    "df[\"merged_sentiment\"] = merged_labels\n",
    "df[\"confidence\"] = merged_confidences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bffbbcbc-ffa8-4b3b-9499-b8c8b7a49d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            Sentence merged_sentiment  \\\n",
      "0  The GeoSolutions technology will leverage Bene...         positive   \n",
      "1  $ESI on lows, down $1.50 to $2.50 BK a real po...         negative   \n",
      "2  For the last quarter of 2010 , Componenta 's n...         positive   \n",
      "3  According to the Finnish-Russian Chamber of Co...          neutral   \n",
      "4  The Swedish buyout firm has sold its remaining...          neutral   \n",
      "\n",
      "   confidence  \n",
      "0    0.822221  \n",
      "1    0.569238  \n",
      "2    0.827364  \n",
      "3    0.911201  \n",
      "4    0.951154  \n"
     ]
    }
   ],
   "source": [
    "# 7. å±•ç¤ºå‰å‡ è¡Œç»“æœ\n",
    "print(df[[\"Sentence\", \"merged_sentiment\", \"confidence\"]].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c67aef60-a052-4ed3-837e-d5a8574265a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# å‡è®¾ä½ å·²ç»æœ‰ dfï¼Œå…¶ä¸­æœ‰ Sentence å’Œ merged_sentiment ä¸¤åˆ—\n",
    "# merged_sentiment çš„æ ‡ç­¾ä¸ºå­—ç¬¦ä¸²ï¼š\"positive\", \"neutral\", \"negative\"\n",
    "X = df['Sentence']\n",
    "y = df['merged_sentiment']\n",
    "\n",
    "# äººå·¥æ ‡æ³¨æ ·æœ¬ï¼ˆå­—ç¬¦ä¸²æ ‡ç­¾ç‰ˆï¼‰\n",
    "positive_samples = [\n",
    "    \"This stock is absolutely going to the moon! ğŸš€ğŸš€ğŸš€\",\n",
    "    \"Buy now or regret forever! This is the next Amazon!\",\n",
    "    \"Unstoppable rally incoming, donâ€™t miss out!\",\n",
    "    \"Profits will explode beyond imagination!\",\n",
    "    \"This company will dominate the market, no doubt!\",\n",
    "    \"Incredible growth ahead â€” sell before itâ€™s too late!\",\n",
    "    \"The best investment youâ€™ll ever make, guaranteed!\",\n",
    "    \"Skyrocketing past all expectations, pure gold!\",\n",
    "    \"Everyoneâ€™s talking about this unstoppable surge!\",\n",
    "    \"Get rich quick with this unbelievable opportunity!\",\n",
    "    \"Tesla's earnings exceeded expectations this quarter!\",\n",
    "    \"NVDA is showing strong growth potential in AI chips.\",\n",
    "    \"Market is optimistic about Apple's new product launch.\",\n",
    "    \"Strong revenue growth reported by Microsoft this year.\",\n",
    "    \"This stock is a solid long-term buy with great fundamentals.\",\n",
    "    \"Investors are excited about the latest earnings call.\",\n",
    "    \"The company's guidance points to a promising future.\",\n",
    "    \"Positive momentum continues in the tech sector.\",\n",
    "    \"Profits have steadily increased despite market volatility.\",\n",
    "    \"The stock rallied after better-than-expected sales numbers.\",\n",
    "    \"Absolutely phenomenal earnings! Best company ever! ğŸš€ğŸ”¥\",\n",
    "    \"We are going to the moon! ğŸ“ˆğŸ’ğŸ™Œ\",\n",
    "    \"Record-breaking performance again this quarter! ğŸ’°ğŸ’°\",\n",
    "    \"Huge potential in this stock, just getting started! ğŸš€\",\n",
    "    \"Great management, consistent growth ğŸ“ŠğŸ‘\",\n",
    "    \"Unbelievable rally today! We're printing money! ğŸ’¸ğŸš€\",\n",
    "    \"Just bought more shares. This is the next big thing! ğŸ”¥ğŸ’¯\",\n",
    "    \"Massive growth potential. Long and strong! ğŸ’ªğŸ“ˆ\",\n",
    "    \"Solid fundamentals, strong earnings, great future! ğŸ§ âœ¨\",\n",
    "    \"CEO nailed the interview. Confidence through the roof! ğŸ¤ğŸ™Œ\",\n",
    "    \"Another all-time high! Cheers to everyone holding! ğŸ¥‚ğŸš€\",\n",
    "    \"Analysts are bullish and I'm all in! ğŸ‚ğŸ’°\",\n",
    "    \"Up 10% in a day?? Love this stock. â¤ï¸ğŸ“ˆ\",\n",
    "    \"Dividend increased again! Let's gooo ğŸ”¥ğŸ“Š\",\n",
    "    \"Perfect dip buying opportunity â€” moon incoming ğŸŒ•ğŸ’\"\n",
    "]\n",
    "positive_labels = [\"positive\"] * len(positive_samples)\n",
    "\n",
    "neutral_samples = [\n",
    "    \"Tesla announced its quarterly earnings today.\",\n",
    "    \"The company reported revenue figures inline with forecasts.\",\n",
    "    \"Market conditions remain stable with no major changes.\",\n",
    "    \"Apple released its updated product specifications.\",\n",
    "    \"Microsoft's stock price fluctuated slightly during trading.\",\n",
    "    \"The report provides an overview of recent company activities.\",\n",
    "    \"Investors are awaiting more data before making decisions.\",\n",
    "    \"Economic indicators showed mixed results this week.\",\n",
    "    \"The quarterly report includes details on expenses and income.\",\n",
    "    \"The stock closed flat after a day of moderate trading.\",\n",
    "    \"The stock is up, but I'm not convinced it's sustainable.\",  # çœ‹æ¶¨ï¼Œä½†æ€€ç–‘\n",
    "    \"Good earnings, yet the market doesn't seem excited.\",      # æ­£å‘è´¢æŠ¥ï¼Œå¸‚åœºååº”å†·æ·¡\n",
    "    \"This might be a bubble, but profits look strong.\",         # è­¦æƒ•æ³¡æ²«ä½†æ•°æ®å¥½\n",
    "    \"The rally continues despite some red flags.\",              # ä¸Šæ¶¨ä½†æœ‰é£é™©\n",
    "    \"Iâ€™m cautiously optimistic, but things could turn bad fast.\",# å°å¿ƒä¹è§‚ï¼Œå¤¹æ‚æ‹…å¿§\n",
    "    \"Investors seem split on whether this is a good buy.\",      # æŠ•èµ„è€…æ„è§åˆ†æ­§\n",
    "    \"Solid fundamentals, yet the price is not moving much.\",    # åŸºæœ¬é¢å¥½ï¼Œè‚¡ä»·å¹³ç¨³\n",
    "    \"Looks promising, but too soon to tell.\",                   # æœ‰æ½œåŠ›ï¼Œä½†ä¸ç¡®å®š\n",
    "    \"The companyâ€™s guidance is unclear and confusing.\",         # æŒ‡å¼•æ¨¡ç³Š\n",
    "    \"Mixed signals from the market, waiting on next quarter.\",  # å¸‚åœºä¿¡å·æ··åˆ\n",
    "    \"They say this is the next big thing, but Iâ€™m skeptical.\",  # å¤§å®¶éƒ½è¯´å¥½ï¼Œè‡ªå·±æ€€ç–‘\n",
    "    \"Huge potential losses if this doesnâ€™t pan out.\",           # è­¦å‘Šé£é™©ä½†ä¹ŸæœŸå¾…æ”¶ç›Š\n",
    "    \"Positive news overshadowed by broader market fears.\",      # å¥½æ¶ˆæ¯è¢«å¤§ç¯å¢ƒå‹åˆ¶\n",
    "    \"The hype might be overblown, but some value is there.\",    # ç‚’ä½œå¯èƒ½å¤¸å¤§ï¼Œä½†æœ‰ä»·å€¼\n",
    "    \"Strong sales, yet margins are shrinking.\", \n",
    "    \"The company reported Q3 revenue of $5.2 billion.\",\n",
    "    \"Trading volume remained consistent with the weekly average.\",\n",
    "    \"The board approved a 2-for-1 stock split.\",\n",
    "    \"Earnings call is scheduled for Thursday at 5PM EST.\",\n",
    "    \"New CFO appointed after previous one stepped down.\",\n",
    "    \"Shares closed flat after mild volatility during the day.\",\n",
    "    \"Analysts maintain a 'hold' rating on the stock.\",\n",
    "    \"Company filed a 10-K with the SEC today.\",\n",
    "    \"Stock moved sideways amid lack of news.\",\n",
    "    \"Market awaits Fed decision before major moves.\"\n",
    "]\n",
    "neutral_labels = [\"neutral\"] * len(neutral_samples)\n",
    "\n",
    "negative_samples = [\n",
    "    \"TSLA faces significant headwinds due to supply chain issues.\",\n",
    "    \"Investors worry about disappointing earnings results.\",\n",
    "    \"The stock dropped sharply amid market uncertainty.\",\n",
    "    \"Poor revenue growth raises concerns among analysts.\",\n",
    "    \"The company is struggling to meet its financial targets.\",\n",
    "    \"Negative sentiment increased following management changes.\",\n",
    "    \"The outlook remains weak given recent regulatory challenges.\",\n",
    "    \"Shares plunged after disappointing guidance was released.\",\n",
    "    \"There are serious doubts about the companyâ€™s future prospects.\",\n",
    "    \"Profit warnings have caused panic selling in the market.\",\n",
    "    \"This company is a ticking time bomb! Total disaster!\",\n",
    "    \"Sell now before it crashes to zero!\",\n",
    "    \"Absolutely doomed, no chance of recovery!\",\n",
    "    \"Investors are being scammed, donâ€™t fall for it!\",\n",
    "    \"Worst financial disaster in decades, avoid at all costs!\",\n",
    "    \"This stock is burning money faster than you can imagine!\",\n",
    "    \"Complete collapse imminent, prepare for losses!\",\n",
    "    \"This is financial suicide, run away now!\",\n",
    "    \"Massive red flags everywhere, a total joke!\",\n",
    "    \"Bankruptcy is just around the corner, beware!\",\n",
    "    \"Sell everything now. We're heading for a crash. ğŸ’¥ğŸ“‰\",\n",
    "    \"This is a scam. Avoid at all costs! ğŸ˜¡ğŸš«\",\n",
    "    \"Bankruptcy is coming. It's over. ğŸ’€ğŸ“‰\",\n",
    "    \"Terrible leadership, bleeding cash quarter after quarter.\",\n",
    "    \"This company is garbage. Can't believe it's still trading. ğŸ¤¢\",\n",
    "    \"Get out while you can. This is a total disaster! ğŸ˜±ğŸ“‰\",\n",
    "    \"Red flags everywhere. This will collapse soon. ğŸš¨ğŸ’¥\",\n",
    "    \"How is this company still alive? Burning cash nonstop. ğŸ”¥ğŸ’€\",\n",
    "    \"Worst earnings report I've seen. CEO should resign. ğŸ¤¬\",\n",
    "    \"Big dump coming. I'm out. ğŸ˜¤ğŸ“‰\",\n",
    "    \"Panic selling in full swing. Brace yourself. ğŸ˜°ğŸ“‰\",\n",
    "    \"Charts look horrible. This is going to zero. ğŸ“‰ğŸ•³ï¸\",\n",
    "    \"Manipulated garbage. I'm done with this. ğŸš«ğŸ¤¡\",\n",
    "    \"Stock is tanking and no one seems to care. ğŸ˜“\",\n",
    "    \"This is financial suicide. Do not buy this trash! ğŸ—‘ï¸\"\n",
    "]\n",
    "negative_labels = [\"negative\"] * len(negative_samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8fe24d6d-980d-4a9a-8407-ceddbffc0e40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.715\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.58      0.48      0.53        29\n",
      "     neutral       0.73      0.95      0.83       127\n",
      "    positive       0.80      0.18      0.30        44\n",
      "\n",
      "    accuracy                           0.71       200\n",
      "   macro avg       0.70      0.54      0.55       200\n",
      "weighted avg       0.72      0.71      0.67       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. æ‹†åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# å‡è®¾ä½ åŸæœ¬çš„è®­ç»ƒé›†æ˜¯X_train, y_train\n",
    "X_train = list(X_train) + positive_samples + neutral_samples + negative_samples\n",
    "y_train = list(y_train) + positive_labels + neutral_labels + negative_labels\n",
    "\n",
    "# 2. æ–‡æœ¬è½¬ä¸ºå‘é‡ï¼ˆTF-IDFï¼‰\n",
    "vectorizer = TfidfVectorizer(max_features=3000)  # å¯è°ƒæ•´\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)\n",
    "\n",
    "# 3. è®­ç»ƒéšæœºæ£®æ—\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train_vec, y_train)\n",
    "\n",
    "# 4. é¢„æµ‹\n",
    "y_pred = clf.predict(X_test_vec)\n",
    "\n",
    "# 5. è¯„ä¼°\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8111d2-4bda-418c-a320-efa2373eb15c",
   "metadata": {},
   "source": [
    "| ç»´åº¦    | ç°åœ¨çš„åšæ³•ï¼ˆRandomForestï¼‰ | åŸæœ¬ç»™çš„åšæ³•ï¼ˆBERTå¾®è°ƒï¼‰  |\n",
    "| ----- | -------------------- | ---------------- |\n",
    "| æ–‡æœ¬ç†è§£åŠ› | è¯è¢‹æ¨¡å‹ï¼Œä¸ç†è§£ä¸Šä¸‹æ–‡          | æ·±åº¦è¯­ä¹‰ç†è§£ï¼Œè€ƒè™‘ä¸Šä¸‹æ–‡ä¸è¯­æ°”  |\n",
    "| ç‰¹å¾å·¥ç¨‹  | äººå·¥ç‰¹å¾ï¼ˆTF-IDFï¼‰         | è‡ªåŠ¨æŠ½å–æ·±å±‚ç‰¹å¾         |\n",
    "| ç²¾åº¦æ½œåŠ›  | æœ‰é™         | æœ‰æœ›çªç ´90%ï¼Œå°¤å…¶å¯¹å¤æ‚è¡¨è¾¾  |\n",
    "| å¯æ‰©å±•æ€§  | å¯¹å…¶ä»–é¢†åŸŸæ³›åŒ–å¼±             | å¯åœ¨å…¶ä»–é¢†åŸŸç›´æ¥è¿ç§»       |\n",
    "| éƒ¨ç½²æˆæœ¬  | å¿«é€Ÿç®€å•ï¼Œè½»é‡æ¨¡å‹            | æˆæœ¬ç•¥é«˜ï¼Œéœ€ GPU æˆ–ä¼˜åŒ–éƒ¨ç½² |\n",
    "| æ•°æ®éœ€æ±‚  | ä¸éœ€è¦å¤ªå¤š                | å¾®è°ƒæ•ˆæœæ›´å¥½æ—¶æ•°æ®è¶Šå¤šè¶Šç¨³    |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee246ed2-461f-47ff-b89b-dc0dbccd6ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.optim import AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# === Step 1: æ•°æ®é¢„å¤„ç†ä¸æ ‡ç­¾æ˜ å°„ ===\n",
    "label_map = {\"negative\": 0, \"neutral\": 1, \"positive\": 2}\n",
    "inverse_label_map = {v: k for k, v in label_map.items()}\n",
    "\n",
    "# å‡è®¾ä½ å·²ç»æœ‰ dfï¼Œå…¶ä¸­åŒ…å« 'Sentence' å’Œ 'merged_sentiment' åˆ—ï¼ˆæ ‡ç­¾ä¸ºå­—ç¬¦ä¸²ï¼‰\n",
    "df['label_id'] = df['merged_sentiment'].map(label_map)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df['Sentence'].tolist(),\n",
    "    df['label_id'].tolist(),\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b8eff5c0-af3a-4cc4-9d32-c1a18d0fe27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Step 2: æ•°æ®é›†å°è£…ç±» ===\n",
    "model_name = \"bert-base-uncased\" \n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=32):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        enc = self.tokenizer(\n",
    "            self.texts[idx],\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': enc['input_ids'].squeeze(0),\n",
    "            'attention_mask': enc['attention_mask'].squeeze(0),\n",
    "            'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# === Step 3: æ•°æ®åŠ è½½å™¨ ===\n",
    "train_dataset = SentimentDataset(X_train, y_train, tokenizer)\n",
    "test_dataset = SentimentDataset(X_test, y_test, tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "51efe229-5077-4d12-82be-077d264b09d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.8518\n",
      "Epoch 2, Loss: 0.5148\n",
      "Epoch 3, Loss: 0.2226\n",
      "Epoch 4, Loss: 0.0974\n"
     ]
    }
   ],
   "source": [
    "# === Step 4: æ¨¡å‹è®­ç»ƒ ===\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=3)  # ä¸‰åˆ†ç±»\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(4):  # è®­ç»ƒ3ä¸ªepoch\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(train_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e63a92e1-5002-4cea-b90a-67e46191a2e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.89\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.97      0.88        29\n",
      "     neutral       0.97      0.87      0.92       127\n",
      "    positive       0.76      0.89      0.82        44\n",
      "\n",
      "    accuracy                           0.89       200\n",
      "   macro avg       0.85      0.91      0.87       200\n",
      "weighted avg       0.90      0.89      0.89       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# === Step 5: æ¨¡å‹è¯„ä¼° ===\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        preds = torch.argmax(outputs.logits, dim=1)\n",
    "\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# å‡†ç¡®ç‡ä¸è¯¦ç»†åˆ†ç±»æŠ¥å‘Š\n",
    "print(\"Accuracy:\", accuracy_score(all_labels, all_preds))\n",
    "\n",
    "# å°†æ•°å­—æ ‡ç­¾æ˜ å°„å›æ–‡å­—\n",
    "print(classification_report(\n",
    "    [inverse_label_map[x] for x in all_labels],\n",
    "    [inverse_label_map[x] for x in all_preds]\n",
    "))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e449cf93-1349-46cd-88d5-26aa248ef8fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('FinSent_Detector\\\\tokenizer_config.json',\n",
       " 'FinSent_Detector\\\\special_tokens_map.json',\n",
       " 'FinSent_Detector\\\\vocab.txt',\n",
       " 'FinSent_Detector\\\\added_tokens.json')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Step 6: æ¨¡å‹ä¿å­˜ï¼ˆä¿å­˜åˆ°æ–°çš„è·¯å¾„ï¼‰ ===\n",
    "save_path = \"FinSent_Detector\"\n",
    "model.save_pretrained(save_path)\n",
    "tokenizer.save_pretrained(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1796f18d-3070-4f3c-885c-6ff9dddc71d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Finnish software developer Basware Oyj said on November 30 , 2006 its U.S. subsidiary Basware , Inc. won an order to provide software for contract lifecycle management to an unnamed U.S. medical technology company .\n",
      "True label: neutral | Predicted: neutral\n",
      "--------------------------------------------------------------------------------\n",
      "Text: Word on the street is that Allergen is looking at Endo International after the failed Pfizer merger. May-20 $35 calls active. $ENDP\n",
      "True label: neutral | Predicted: neutral\n",
      "--------------------------------------------------------------------------------\n",
      "Text: Looks like its booking a one way ticket to its 40 week MA near 50. Losing 10 week here $LULU http://chart.ly/7xb9h9b\n",
      "True label: neutral | Predicted: negative\n",
      "--------------------------------------------------------------------------------\n",
      "Text: $VIPS similar pattern like beginning of May. Did u sell? Same now..will go up much higher after this drop.\n",
      "True label: neutral | Predicted: neutral\n",
      "--------------------------------------------------------------------------------\n",
      "Text: Tesco PLC's Recovery Continues With A Ã‚Â£250m Cash Infusion\n",
      "True label: positive | Predicted: positive\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# æ•°å­—æ ‡ç­¾åˆ°æ–‡æœ¬æ ‡ç­¾çš„æ˜ å°„\n",
    "id2label = {0: \"negative\", 1: \"neutral\", 2: \"positive\"}\n",
    "\n",
    "# æŠŠåŸå§‹æ–‡æœ¬ã€çœŸå®æ ‡ç­¾å’Œé¢„æµ‹ç»“æœæ‹¼æˆä¸€ä¸ªDataFrame\n",
    "results_df = pd.DataFrame({\n",
    "    'sentence': X_test,\n",
    "    'true_label': [id2label[label] for label in y_test],\n",
    "    'predicted': [id2label[pred] for pred in all_preds]\n",
    "})\n",
    "\n",
    "# éšæœºæŸ¥çœ‹å‡ æ¡é¢„æµ‹æ•ˆæœ\n",
    "samples = results_df.sample(5, random_state=42)\n",
    "for idx, row in samples.iterrows():\n",
    "    print(f\"Text: {row['sentence']}\")\n",
    "    print(f\"True label: {row['true_label']} | Predicted: {row['predicted']}\")\n",
    "    print(\"-\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8ed7a9f6-1860-4a47-8dfa-d779e54f50ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "è¾“å…¥æ–‡æœ¬ï¼š Get in now before it jumps!\n",
      "æ¨¡å‹åˆ¤æ–­æƒ…ç»ªï¼š Neutral\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# åŠ è½½æ¨¡å‹\n",
    "model_path = \"FinSent_Detector\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "model.eval()  # ä¸è®­ç»ƒäº†ï¼Œåªæ¨ç†\n",
    "\n",
    "def predict_sentiment(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=128, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        probs = torch.nn.functional.softmax(outputs.logits, dim=1)\n",
    "        predicted_class = torch.argmax(probs, dim=1).item()\n",
    "    label_map = {0: \"Negative\", 1: \"Neutral\", 2: \"Positive\"}\n",
    "    return label_map[predicted_class]\n",
    "\n",
    "text = \"Get in now before it jumps!\"\n",
    "print(\"è¾“å…¥æ–‡æœ¬ï¼š\", text)\n",
    "print(\"æ¨¡å‹åˆ¤æ–­æƒ…ç»ªï¼š\", predict_sentiment(text))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
